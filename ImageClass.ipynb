{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:20:32.644936Z",
     "end_time": "2023-04-22T17:20:35.704550Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "test_set.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T15:38:14.792153Z",
     "end_time": "2023-04-22T15:38:15.137923Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_counts = train_data[\"label\"].value_counts()\n",
    "\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "\n",
    "plt.xlabel(\"Number\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Label target class\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:25:46.361644Z",
     "end_time": "2023-04-22T17:25:47.198132Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data set is a collection of handwritten numbers. The models should be able to predict the handwritten numbers into the actual numerical digit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "237/237 [==============================] - 1s 4ms/step - loss: 0.4357 - accuracy: 0.8748 - val_loss: 0.2234 - val_accuracy: 0.9426\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.2037 - accuracy: 0.9407 - val_loss: 0.1625 - val_accuracy: 0.9577\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9562 - val_loss: 0.1286 - val_accuracy: 0.9670\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.1172 - accuracy: 0.9654 - val_loss: 0.1207 - val_accuracy: 0.9649\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9716 - val_loss: 0.1020 - val_accuracy: 0.9699\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0813 - accuracy: 0.9764 - val_loss: 0.0917 - val_accuracy: 0.9753\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0699 - accuracy: 0.9793 - val_loss: 0.0892 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0594 - accuracy: 0.9827 - val_loss: 0.0846 - val_accuracy: 0.9780\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9854 - val_loss: 0.0839 - val_accuracy: 0.9756\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9873 - val_loss: 0.0771 - val_accuracy: 0.9786\n",
      "263/263 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9751\n",
      "Test loss: 0.08643923699855804\n",
      "Test accuracy: 0.9751190543174744\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = train_set.drop('label', axis=1).values.astype(np.float32)\n",
    "y_train = keras.utils.to_categorical(train_set['label'].values, 10)\n",
    "\n",
    "X_test = test_set.drop('label', axis=1).values.astype(np.float32)\n",
    "y_test = keras.utils.to_categorical(test_set['label'].values, 10)\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "seq_model = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "seq_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = seq_model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = seq_model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:31:55.422182Z",
     "end_time": "2023-04-22T17:32:03.712435Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "237/237 [==============================] - 6s 22ms/step - loss: 0.3822 - accuracy: 0.8930 - val_loss: 0.1572 - val_accuracy: 0.9571\n",
      "Epoch 2/10\n",
      "237/237 [==============================] - 6s 27ms/step - loss: 0.1225 - accuracy: 0.9651 - val_loss: 0.0956 - val_accuracy: 0.9738\n",
      "Epoch 3/10\n",
      "237/237 [==============================] - 6s 24ms/step - loss: 0.0796 - accuracy: 0.9760 - val_loss: 0.0850 - val_accuracy: 0.9783\n",
      "Epoch 4/10\n",
      "237/237 [==============================] - 5s 20ms/step - loss: 0.0580 - accuracy: 0.9828 - val_loss: 0.0739 - val_accuracy: 0.9801\n",
      "Epoch 5/10\n",
      "237/237 [==============================] - 5s 23ms/step - loss: 0.0441 - accuracy: 0.9869 - val_loss: 0.0643 - val_accuracy: 0.9815\n",
      "Epoch 6/10\n",
      "237/237 [==============================] - 5s 21ms/step - loss: 0.0359 - accuracy: 0.9890 - val_loss: 0.0691 - val_accuracy: 0.9807\n",
      "Epoch 7/10\n",
      "237/237 [==============================] - 5s 22ms/step - loss: 0.0303 - accuracy: 0.9910 - val_loss: 0.0663 - val_accuracy: 0.9818\n",
      "Epoch 8/10\n",
      "237/237 [==============================] - 6s 24ms/step - loss: 0.0246 - accuracy: 0.9931 - val_loss: 0.0632 - val_accuracy: 0.9798\n",
      "Epoch 9/10\n",
      "237/237 [==============================] - 7s 31ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.0670 - val_accuracy: 0.9804\n",
      "Epoch 10/10\n",
      "237/237 [==============================] - 9s 36ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 0.0555 - val_accuracy: 0.9854\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0552 - accuracy: 0.9830\n",
      "Test loss: 0.05516010522842407\n",
      "Test accuracy: 0.9829761981964111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "X_train = train_set.drop('label', axis=1).values.astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "y_train = keras.utils.to_categorical(train_set['label'].values, 10)\n",
    "\n",
    "X_test = test_set.drop('label', axis=1).values.astype(np.float32).reshape(-1, 28, 28, 1)\n",
    "y_test = keras.utils.to_categorical(test_set['label'].values, 10)\n",
    "\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "cnn_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:33:26.521757Z",
     "end_time": "2023-04-22T17:34:28.740459Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "response = urllib.request.urlopen(url, context=context)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:09:15.844401Z",
     "end_time": "2023-04-22T17:09:16.263905Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "filename = 'cats_and_dogs_filtered.zip'\n",
    "if not os.path.exists(filename):\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(url, context=context)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.read())\n",
    "\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dir = 'cats_and_dogs_filtered/train'\n",
    "val_dir = 'cats_and_dogs_filtered/validation'\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T17:22:42.888082Z",
     "end_time": "2023-04-22T17:22:44.901403Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 17:15:37.368049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/200 [==>...........................] - ETA: 13:55 - loss: 0.4820 - accuracy: 0.7513"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[88], line 37\u001B[0m\n\u001B[1;32m     34\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39mAdam(lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m), metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_generator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/keras/engine/training.py:1685\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1677\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1678\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1679\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1682\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1683\u001B[0m ):\n\u001B[1;32m   1684\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1685\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1687\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    891\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 894\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    896\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    897\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    923\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    924\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    925\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 926\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    927\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    928\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    929\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    930\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    141\u001B[0m   (concrete_function,\n\u001B[1;32m    142\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1753\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1755\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1756\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1757\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1759\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1760\u001B[0m     args,\n\u001B[1;32m   1761\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1762\u001B[0m     executing_eagerly)\n\u001B[1;32m   1763\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    380\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 381\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    388\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    390\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    393\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    394\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/PycharmProjects/ImageClassification/venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "url = 'https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "filename = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "response = urllib.request.urlopen(url, context=context)\n",
    "with open(filename, 'wb') as f:\n",
    "    f.write(response.read())\n",
    "\n",
    "vgg = VGG16(include_top=False, weights=filename, input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=predictions)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=200,\n",
    "        epochs=10,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=50)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis:\n",
    "By computing all the accuracy we notice that the CNN architecture is producing the best accuracy. However all the accuracies are really close to each other and it is about 95-98%. This proves that all the models are creating very good predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
